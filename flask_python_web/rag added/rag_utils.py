import psycopg2
import numpy as np
import requests
from sentence_transformers import SentenceTransformer

# ğŸ”¹ Embedding modeli (384 boyutlu)
model = SentenceTransformer("all-MiniLM-L6-v2")

# ğŸ”¹ VeritabanÄ± baÄŸlantÄ± ayarlarÄ±
DB_CONFIG = {
    "dbname": "ragdb",
    "user": "postgres",
    "password": "secret",  # kendi ÅŸifreni kullan
    "host": "localhost",
    "port": "5432"
}

# ğŸ”¹ Ollama ayarlarÄ±
OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "turkcell-custom"  # model ismini deÄŸiÅŸtir

def get_similar_chunks(question, top_k=3):
    try:
        # Embedding oluÅŸtur
        embedding = model.encode(question, convert_to_tensor=True).cpu().numpy().tolist()
        
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()

        # GeliÅŸtirilmiÅŸ sorgu
        cur.execute("""
            SELECT content, 1 - (embedding <=> %s::vector) as similarity 
            FROM documents
            WHERE 1 - (embedding <=> %s::vector) > 0.3
            ORDER BY similarity DESC
            LIMIT %s;
        """, (embedding, embedding, top_k))

        results = [row[0] for row in cur.fetchall()]
        print(f"ğŸ” Bulunan {len(results)} benzer dokÃ¼man")
        return results
        
    except Exception as e:
        print(f"âŒ VeritabanÄ± HatasÄ±: {str(e)}")
        return []

def ask_ollama(payload_or_prompt):
    try:
        if isinstance(payload_or_prompt, dict):
            payload = payload_or_prompt
        else:
            payload = {
                "model": OLLAMA_MODEL,
                "prompt": payload_or_prompt,
                "stream": False
            }

        print("ğŸ“¤ [ask_ollama] Prompt gÃ¶nderiliyor...")
        response = requests.post(OLLAMA_URL, json=payload)
        if response.status_code == 200:
            result = response.json().get("response", "")
            print("âœ… [ask_ollama] YanÄ±t alÄ±ndÄ±:", result[:100])
            return result
        else:
            print("âŒ [ask_ollama] HTTP HatasÄ±:", response.status_code)
            return f"âš ï¸ Ollama HatasÄ±: {response.status_code}"
    except Exception as e:
        print("âŒ [ask_ollama] HATA:", str(e))
        return f"âš ï¸ Ollama HatasÄ±: {str(e)}"

def rag_ask(question):
    """TÃ¼m RAG zincirini Ã§alÄ±ÅŸtÄ±r."""
    print("\nğŸ” Soru alÄ±ndÄ±:", question)
    chunks = get_similar_chunks(question)
    if not chunks:
        print("âš ï¸ HiÃ§ benzer iÃ§erik bulunamadÄ±.")
        return
    prompt = build_prompt(question, chunks)
    print("\nğŸ“‹ OluÅŸturulan prompt:\n", prompt[:600], "...\n")
    answer = ask_ollama(prompt)
    print("ğŸ¤– Cevap:\n", answer)


# rag_utils.py'ye bu fonksiyonu ekleyin
def build_prompt(question, context_chunks):
    """RAG iÃ§in prompt oluÅŸturur"""
    context = "\n".join([f"- {chunk}" for chunk in context_chunks])
    
    return f"""<|system|>
AÅŸaÄŸÄ±daki baÄŸlam bilgilerini kullanarak soruyu yanÄ±tlayÄ±n. 
EÄŸer cevap verilen bilgilerde yoksa "Bu konuda yeterli bilgi bulunmamaktadÄ±r." yazÄ±n.

BaÄŸlam Bilgileri:
{context}

Soru: {question}
Cevap:"""



# ğŸ”¹ Terminalden Ã§alÄ±ÅŸtÄ±rmak iÃ§in
if __name__ == "_main_":
    while True:
        question = input("\nğŸ‘¤ Soru (Ã§Ä±kmak iÃ§in 'q'): ")
        if question.lower() == "q":
            break
        rag_ask(question)